<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>Natural Language Generation using Generative Adversarial Networks - Akash Saravanan</title><link rel=apple-touch-icon sizes=180x180 href=https://akashsara.github.io/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://akashsara.github.io/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://akashsara.github.io/favicon-16x16.png><link rel=manifest href=https://akashsara.github.io/site.webmanifest><link rel=mask-icon href=https://akashsara.github.io/safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#da532c"><meta name=theme-color content="#ffffff"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:title" content="Natural Language Generation using Generative Adversarial Networks"><meta property="og:description" content="Performed a comparative study of existing text generation methodologies and established a baseline performance using different models. Each model was trained on:
”Alice in Wonderland” ”The Adventures of Sherlock Holmes” The models we utilized for training included:
Classic LSTM Networks. A Sequence-to-Sequence Model using LSTMs & GRUs. A Transformer with Attention. Developed a novel Generative Adversarial Network (GAN) using an LSTM generator and a fully connected discriminator.
While the network generated coherent sentences, sequence models and transformers retained the best performance"><meta property="og:type" content="article"><meta property="og:url" content="https://akashsara.github.io/publications/nlg-gan/"><meta property="article:section" content="publications"><meta property="article:published_time" content="2019-05-01T16:36:11+04:30"><meta property="article:modified_time" content="2019-05-01T16:36:11+04:30"><meta name=twitter:card content="summary"><meta name=twitter:title content="Natural Language Generation using Generative Adversarial Networks"><meta name=twitter:description content="Performed a comparative study of existing text generation methodologies and established a baseline performance using different models. Each model was trained on:
”Alice in Wonderland” ”The Adventures of Sherlock Holmes” The models we utilized for training included:
Classic LSTM Networks. A Sequence-to-Sequence Model using LSTMs & GRUs. A Transformer with Attention. Developed a novel Generative Adversarial Network (GAN) using an LSTM generator and a fully connected discriminator.
While the network generated coherent sentences, sequence models and transformers retained the best performance"><link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:500,100,300" rel=stylesheet><link rel=stylesheet type=text/css media=screen href=https://akashsara.github.io/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://akashsara.github.io/css/main.css><script src=https://akashsara.github.io/js/feather.min.js></script><script src=https://akashsara.github.io/js/main.js></script></head><body><div id=content><div class="header wrapper"><span class=page_title>Natural Language Generation Using Generative Adversarial Networks</span><ul class=flat><li class=page_meta>May 1, 2019</li></ul><a href=https://akashsara.github.io/ title=Home class=page_meta>Return Home</a></div><div class="content wrapper"></div></div><div class="footer wrapper"><nav class=nav><div><a href=https://github.com/hadisinaee/avicenna>Avicenna Theme</a> | Built with <a href=https://gohugo.io>Hugo</a></div></nav></div><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-137697069-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script>
<script>feather.replace()</script></body></html>