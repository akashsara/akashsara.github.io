+++
title = "Natural Language Generation using Generative Adversarial Networks"
date = 2019-05-01T16:36:11+04:30
draft = false
+++

Performed a comparative study of existing text generation methodologies and established a baseline performance using different models.
Each model was trained on:

1. ”Alice in Wonderland” 
2. ”The Adventures of Sherlock Holmes”  

The models we utilized for training included:

1. Classic LSTM Networks.
2. A Sequence-to-Sequence Model using LSTMs & GRUs.
3. A Transformer with Attention.

Developed a novel Generative Adversarial Network (GAN) using an LSTM generator and a fully connected discriminator.  

While the network generated coherent sentences, sequence models and transformers retained the best performance